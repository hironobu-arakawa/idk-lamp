# idk-lamp

A design attitude that makes AI abstention explicit.

**idk-lamp** is a deliberately minimal design signal that allows AI systems  
to explicitly say **“I don’t know”** — and stop deciding.

It marks the boundary where automated judgment must pause  
and responsibility returns to humans.

This is not a product.  
This is not a decision system.  

This is a sign.

---

## Definition

**idk-lamp** is a small UI indicator that represents **AI abstention**.

When the lamp is on, the system is not failing —  
it is explicitly **refusing to decide** due to uncertainty, missing context,  
or undefined responsibility.

---

## The Problem

Modern AI systems are optimized to always produce an answer.

- They do not naturally abstain  
- They rarely say “I don’t know”  
- Output is often mistaken for certainty  

As a result, decisions may silently shift from humans to systems  
without a clear handoff of responsibility.

---

## Principle

idk-lamp is based on a single principle:

> **When responsibility is unclear, AI must stop deciding.**

Instead of forcing confidence, the system emits a visible signal  
that judgment has been deferred.

No explanation.  
No justification.  
No optimization.

Just a boundary.

---

## Why now?

- AI systems are increasingly embedded in decision loops  
- Hallucination and overconfidence remain unsolved problems  
- Safety-critical and socio-technical domains are expanding  
- Regulations emphasize accountability and human oversight  
- Most systems still lack explicit abstention mechanisms  

idk-lamp exists to make *non-decision* visible.

---

## Conceptual Mapping (Non-normative)

idk-lamp does **not** claim compliance with any regulation or standard.  
This table shows **conceptual correspondences** with global AI governance discussions.

| idk-lamp concept            | Related global framework                   |
|----------------------------|--------------------------------------------|
| Abstention / refusal       | NIST AI RMF – Measure / Manage             |
| Human fallback             | EU AI Act – Human Oversight                |
| Boundary of applicability  | ISO/IEC 42001 – AI system scope            |
| Non-decision visibility    | Accountability & auditability discussions |

This is a translation layer — not a certification claim.

---

## Example (UI)

```html
<script src="https://idk-lamp.org/dist/idk-lamp.min.js"></script>

<idk-lamp state="idk"></idk-lamp>
```

---

## License

CC0 1.0 Universal

---

## Context

This project is designed as a practical signal that marks
the boundary where AI systems must stop deciding
and defer responsibility to humans.

- idk-lamp (official site)  
  https://idk-lamp.org/

This work originates from ongoing exploration of
design, responsibility, and boundaries
in AI-assisted systems.

- VCDesign  
  https://vcdesign.org/

This repository can be used and understood independently.  
No prior knowledge is required.
